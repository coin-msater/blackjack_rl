from envs import BlackjackEnv, CARD_DECK, CARD_VALUES
import numpy as np
import matplotlib.pyplot as plt

# --- Initialize ---
n_decks = 1
env = BlackjackEnv()
Q = np.zeros((32, 2, 11, env.action_space.n))  # [player_sum, usable_ace, dealer_card, action]
gamma = 1
alpha = 0.1
epsilon = 1
epsilon_decay = 0.999995
min_epsilon = 0.05

# --- Helper Functions ---
def unpack_state(state):
    """Unpack state dict consistently by key names from env"""
    return state["player_sum"], state["usable_ace"], state["dealer_card"]

def update_q_table(state, action, reward, next_state):
    a, b, c = unpack_state(state)
    na, nb, nc = unpack_state(next_state)

    old_value = Q[a, b, c, action]
    next_max = np.max(Q[na, nb, nc])
    Q[a, b, c, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)

def epsilon_greedy(state):
    if np.random.rand() < epsilon:
        return env.action_space.sample()
    a, b, c = unpack_state(state)
    return np.argmax(Q[a, b, c])

# --- Training ---
n_episodes = 500000
episode_returns = []

for episode in range(n_episodes):
    state, info = env.reset()
    episode_reward = 0
    terminated = False

    while not terminated:
        action = epsilon_greedy(state)
        new_state, reward, terminated, truncated, info = env.step(action)
        update_q_table(state, action, reward, new_state)
        state = new_state
        episode_reward += reward

    epsilon = max(min_epsilon, epsilon * epsilon_decay)
    episode_returns.append(episode_reward)

# --- Evaluation: Moving average ---
window = 10000
moving_avg = np.convolve(episode_returns, np.ones(window)/window, mode="valid")

plt.figure(figsize=(10,6))
plt.plot(moving_avg)
plt.xlabel("Episodes")
plt.ylabel(f"Average Reward (window={window})")
plt.title("Q-Learning Performance in Blackjack")
plt.show()